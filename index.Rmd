---
title: "Weight Lifting Exercise"
author: "LateNightCoder0815"
date: "19 Juni 2018"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Executive summary
In this report we build a machine learning model to predict the manner in which the [weight lifting exercise](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) has been performed. We use state vector machine and random forest models to choose the most accurate method on the cross validation sample. We are able to build an "optimal" model where we expect no out of sample error. 

## Download and cleaning the dataset

First we will download the dataset from the webpage. We use the dataset on "Qualitative Activity Recognition of Weight Lifting Exercises" provided by [Groupware](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).

```{r}
## Load required libraries
suppressWarnings(suppressMessages(library(e1071)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(randomForest)))

## Define file names for download of datasets
fileNameTrain <- 'pml-training.csv'
fileNameTesting <- 'pml-testing.csv'

## Download dataset if not exist
if (!file.exists(fileNameTrain)){
  download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
                fileNameTrain)
}

## Download dataset if not exist
if (!file.exists(fileNameTesting)){
  download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',
                fileNameTesting)
}

training <- read.csv(fileNameTrain)
testing <- read.csv(fileNameTesting)

```

There are many variables inside the dataset only containing missing values. We will choose only the variables which have values within the testing set to build our prediction model. Additionally, we can see that the dataset contains multiple technical values, which should be removed from our features list. This includes the row number, user name, time stamps, technical ids as well as the problem_id from the testing set.

```{r}
# Calculate the sum of NA Element within every column of the test data
features <- apply(testing,2,function(x) sum(is.na(x)))

# Only include variables where we have values in the test data
features <- names(features[features == 0])

# Remove row number, user name, time stamps and technical ids as well as 
# problem_id from testing data set
features <- features[8:59]

# Build tidy datasets / training includes the labels classe
training <- training[,c(features,'classe')] 
testing <- testing[,features]
```

We will use the following features in our model:

```{r}
features
```

For cross validation we will create an additional validation set (10%) from the training data.

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.9, list=FALSE)

training <- training[inTrain,]
val <- training[-inTrain,]
```


## Exploratory Data Analysis

For initial exploration of the data we use PCA.

```{r}
trainPCA <- prcomp(training[,features])

qplot(trainPCA$x[,1],trainPCA$x[,2],col=training$classe,xlab='PC1',ylab='PC2')
```

From the chart above we can already see a certain structure within the data. Nevertheless, two components do not seem to suffice in order to build a prediction model.

```{r}
summary(trainPCA)
```

From the summary we can conclude that we need at least 7 components to cover 90% of the overall variance. This can be used if we run into computational issues with our machine learning models for data compression.



## Prediction models and cross validation

### State Vector Machine

We will use the state vector machine algorithm to build our initial prediction model. To measure the expected out of sample error we will use cross validation.

```{r}
# Fit SVM model on training data
fitSVM <- svm(classe ~.,data=training)

# Make predictions on training and validation set
predTrain <- predict(fitSVM,training)
predVal <- predict(fitSVM,val)

# Calculate accuracy of the model on train and validation set
accTrainSVM <- confusionMatrix(predTrain,training$classe)$overall[1]
accValSVM <- confusionMatrix(predVal,val$classe)$overall[1]
```
Using the svm model we already obtain very high accuracy on the training (accuracy: `r accTrainSVM`) and on the cross-validation set (accuracy: `r accValSVM`). This result may get improved by feature scaling even further as the svm is sensitive to the distribution of features.

### Random forests

We conduct the same experiment with the random forest algorithm.
```{r}
# Fit rf model on training data
fitRF <- randomForest(classe ~.,data=training)

# Make predictions on training and validation set
predTrain <- predict(fitRF,training)
predVal <- predict(fitRF,val)

# Calculate accuracy of the model on train and validation set
accTrainRF <- confusionMatrix(predTrain,training$classe)$overall[1]
accValRF <- confusionMatrix(predVal,val$classe)$overall[1]
```
Using the random forest model we obtain even higher accuracy and seem to be able to perfectly predict on the training (accuracy: `r accTrainRF`) as well as on the cross-validation set (accuracy: `r accValRF`). Our estimation is that we do not have any out of sample error.


## Predict on testing dataset

We use the strongest model (RF measured by accuracy) to predict on the testing data for our final prediction, which we submit to the system.
```{r}
predTest <- predict(fitRF,testing)
predTest
```